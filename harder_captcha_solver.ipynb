{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import cv2\n",
    "import glob\n",
    "import imutils\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten, Dense\n",
    "from imutils import paths\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_to_fit(image, width, height):\n",
    "    \"\"\"\n",
    "    A helper function to resize an image to fit within a given size\n",
    "    :param image: image to resize\n",
    "    :param width: desired width in pixels\n",
    "    :param height: desired height in pixels\n",
    "    :return: the resized image\n",
    "    \"\"\"\n",
    "\n",
    "    # grab the dimensions of the image, then initialize the padding values\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # if the width is greater than the height then resize along the width\n",
    "    if w > h:\n",
    "        image = imutils.resize(image, width=width)\n",
    "\n",
    "    # otherwise, the height is greater than the width so resize along the height\n",
    "    else:\n",
    "        image = imutils.resize(image, height=height)\n",
    "\n",
    "    # determine the padding values for the width and height to obtain the target dimensions\n",
    "    padW = int((width - image.shape[1]) / 2.0)\n",
    "    padH = int((height - image.shape[0]) / 2.0)\n",
    "\n",
    "    # pad the image then apply one more resizing to handle any rounding issues\n",
    "    image = cv2.copyMakeBorder(image, padH, padH, padW, padW,cv2.BORDER_REPLICATE)\n",
    "    image = cv2.resize(image, (width, height))\n",
    "\n",
    "    # return the pre-processed image\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "# loop over the input images\n",
    "for image_file in paths.list_images('kaggle-letters'):\n",
    "    # Load the image and convert it to grayscale\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize the letter so it fits in a 20x20 pixel box\n",
    "    image = resize_to_fit(image, 20, 20)\n",
    "\n",
    "    # Add a third channel dimension to the image to make Keras work\n",
    "    image = np.expand_dims(image, axis=2)\n",
    "\n",
    "    # Grab the name of the letter based on the folder it was in\n",
    "    label = image_file.split(os.path.sep)[-2]\n",
    "\n",
    "    # Add the letter image and it's label to our training data\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "# data[0].shape -> (20, 20, 1)\n",
    "\n",
    "# Scale the raw pixel intensities to [0,1] -> improves training\n",
    "data = np.array(data, dtype='float') / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size = 0.3, random_state=0, stratify=labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state=0, stratify=y_train)\n",
    "\n",
    "# Convert the labels (letters) into one-hot-encodings that Keras can work with\n",
    "# Used to extend classifiers to multi-class classifiers. It will make the model consisting of \"one model per class\n",
    "labelbin = LabelBinarizer().fit(y_train)\n",
    "y_train = labelbin.transform(y_train)\n",
    "y_val = labelbin.transform(y_val)\n",
    "y_test = labelbin.transform(y_test)\n",
    "\n",
    "# Save mapping from labels to one-hot-encodings\n",
    "with open(\"hard_labels.dat\", 'wb') as f:\n",
    "    pickle.dump(labelbin, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-12 00:35:07.485491: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-12 00:35:07.486548: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# 1st convolutional layer with max pooling -> 20 kernels\n",
    "model.add(Conv2D(20, (5, 5), padding=\"same\", input_shape=(20, 20, 1), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# 2nd convolutional layer with max pooling -> 50 kernels\n",
    "model.add(Conv2D(50, (5, 5), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully Connected network\n",
    "# Hidden layer with 500 nodes\n",
    "model.add(Dense(500, activation = 'relu'))\n",
    "\n",
    "# Output layer with 32 nodes -> number of possible characters\n",
    "model.add(Dense(62, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1700/1700 [==============================] - 117s 69ms/step - loss: 0.5896 - accuracy: 0.8513 - val_loss: 0.2069 - val_accuracy: 0.9548\n",
      "Epoch 2/10\n",
      "1700/1700 [==============================] - 112s 66ms/step - loss: 0.1722 - accuracy: 0.9591 - val_loss: 0.1539 - val_accuracy: 0.9674\n",
      "Epoch 3/10\n",
      "1700/1700 [==============================] - 127s 75ms/step - loss: 0.1188 - accuracy: 0.9714 - val_loss: 0.1299 - val_accuracy: 0.9687\n",
      "Epoch 4/10\n",
      "1700/1700 [==============================] - 126s 74ms/step - loss: 0.0888 - accuracy: 0.9786 - val_loss: 0.1092 - val_accuracy: 0.9791\n",
      "Epoch 5/10\n",
      "1700/1700 [==============================] - 127s 74ms/step - loss: 0.0694 - accuracy: 0.9824 - val_loss: 0.1160 - val_accuracy: 0.9740\n",
      "Epoch 6/10\n",
      "1700/1700 [==============================] - 129s 76ms/step - loss: 0.0563 - accuracy: 0.9852 - val_loss: 0.1040 - val_accuracy: 0.9805\n",
      "Epoch 7/10\n",
      "1700/1700 [==============================] - 126s 74ms/step - loss: 0.0504 - accuracy: 0.9863 - val_loss: 0.0944 - val_accuracy: 0.9828\n",
      "Epoch 8/10\n",
      "1700/1700 [==============================] - 129s 76ms/step - loss: 0.0401 - accuracy: 0.9894 - val_loss: 0.1013 - val_accuracy: 0.9810\n",
      "Epoch 9/10\n",
      "1700/1700 [==============================] - 132s 78ms/step - loss: 0.0394 - accuracy: 0.9893 - val_loss: 0.1050 - val_accuracy: 0.9839\n",
      "Epoch 10/10\n",
      "1700/1700 [==============================] - 133s 78ms/step - loss: 0.0350 - accuracy: 0.9904 - val_loss: 0.1008 - val_accuracy: 0.9848\n"
     ]
    }
   ],
   "source": [
    "# Train the neural network\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size = 32, epochs = 10, verbose = 1)\n",
    "\n",
    "model.save('hard_captchas_solver.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load label enconder\n",
    "with open('hard_labels.dat', 'rb') as f:\n",
    "    lb2 = pickle.load(f)\n",
    "\n",
    "# load CNN\n",
    "model = load_model('hard_captchas_solver.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810/810 [==============================] - 10s 12ms/step - loss: 0.0902 - accuracy: 0.9837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09021096676588058, 0.9837002754211426]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, batch_size=32)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning to classify the simpler captcha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the old model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 20, 20, 20)        520       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 10, 10, 20)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 50)        25050     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 50)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1250)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 500)               625500    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                16032     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 667,102\n",
      "Trainable params: 641,532\n",
      "Non-trainable params: 25,570\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = Sequential()\n",
    "\n",
    "# add all but the last two layers of the previous model\n",
    "for layer in model.layers[:-2]:\n",
    "    new_model.add(layer)\n",
    "\n",
    "# stop convolutional layers weights from being updated\n",
    "for layer in new_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# add two last layers\n",
    "new_model.add(Dense(500, activation = 'relu'))\n",
    "\n",
    "# Output layer with 32 nodes -> number of possible characters\n",
    "new_model.add(Dense(32, activation = 'softmax'))\n",
    "\n",
    "new_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics =['accuracy'])\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "with open('simple_labels.dat', 'rb') as f:\n",
    "    lb = pickle.load(f)\n",
    "\n",
    "# loop over the input images\n",
    "for image_file in paths.list_images('extracted_letter_images'):\n",
    "    # Load the image and convert it to grayscale\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize the letter so it fits in a 20x20 pixel box\n",
    "    image = resize_to_fit(image, 20, 20)\n",
    "\n",
    "    # Add a third channel dimension to the image to make Keras work\n",
    "    image = np.expand_dims(image, axis=2)\n",
    "\n",
    "    # Grab the name of the letter based on the folder it was in\n",
    "    label = image_file.split(os.path.sep)[-2]\n",
    "\n",
    "    # Add the letter image and it's label to our training data\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "# data[0].shape -> (20, 20, 1)\n",
    "\n",
    "# Scale the raw pixel intensities to [0,1] -> improves training\n",
    "data = np.array(data, dtype='float') / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "X_train, X_test_simple, y_train, y_test_simple = train_test_split(data, labels, test_size = 0.30, random_state=0, stratify=labels)\n",
    "\n",
    "X_train_simple, X_val_simple, y_train_simple, y_val_simple = train_test_split(X_train, y_train, test_size = 0.10, random_state=0, stratify=y_train)\n",
    "\n",
    "y_train_simple = lb.transform(y_train_simple)\n",
    "y_val_simple = lb.transform(y_val_simple)\n",
    "y_test_simple = lb.transform(y_test_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "763/763 [==============================] - 20s 26ms/step - loss: 0.0793 - accuracy: 0.9823 - val_loss: 0.0368 - val_accuracy: 0.9915\n",
      "Epoch 2/10\n",
      "763/763 [==============================] - 19s 25ms/step - loss: 0.0110 - accuracy: 0.9968 - val_loss: 0.0265 - val_accuracy: 0.9948\n",
      "Epoch 3/10\n",
      "763/763 [==============================] - 21s 28ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0193 - val_accuracy: 0.9948\n",
      "Epoch 4/10\n",
      "763/763 [==============================] - 21s 27ms/step - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.0239 - val_accuracy: 0.9963\n",
      "Epoch 5/10\n",
      "763/763 [==============================] - 21s 28ms/step - loss: 0.0110 - accuracy: 0.9974 - val_loss: 0.0236 - val_accuracy: 0.9963\n",
      "Epoch 6/10\n",
      "763/763 [==============================] - 21s 28ms/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.0282 - val_accuracy: 0.9956\n",
      "Epoch 7/10\n",
      "763/763 [==============================] - 21s 27ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.0276 - val_accuracy: 0.9952\n",
      "Epoch 8/10\n",
      "763/763 [==============================] - 21s 28ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.0196 - val_accuracy: 0.9963\n",
      "Epoch 9/10\n",
      "763/763 [==============================] - 21s 28ms/step - loss: 0.0053 - accuracy: 0.9987 - val_loss: 0.0558 - val_accuracy: 0.9937\n",
      "Epoch 10/10\n",
      "763/763 [==============================] - 21s 28ms/step - loss: 0.0071 - accuracy: 0.9983 - val_loss: 0.0545 - val_accuracy: 0.9930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc98ac21bb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.fit(X_train_simple, y_train_simple, validation_data=(X_val_simple, y_val_simple), batch_size = 32, epochs = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.save('transfer_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 4s 12ms/step - loss: 0.0415 - accuracy: 0.9948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.041542135179042816, 0.9947522282600403]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = new_model.evaluate(X_test_simple, y_test_simple, batch_size=32)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the results in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_captcha_test_preprocessing(image_file):\n",
    "\n",
    "    # Load image\n",
    "    img = cv2.imread(image_file)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.copyMakeBorder(img, 20, 20, 20, 20, cv2.BORDER_REPLICATE)\n",
    "\n",
    "    img_thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    img_contours = cv2.findContours(img_thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    img_contours = img_contours[1] if imutils.is_cv3() else img_contours[0]\n",
    "\n",
    "    letter_image_regions = []\n",
    "\n",
    "    for contour in img_contours:\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "\n",
    "        if w / h > 1.25:\n",
    "            half_width = int(w/2)\n",
    "            letter_image_regions.append((x, y, half_width, h))\n",
    "            letter_image_regions.append((x + half_width, y, half_width, h))\n",
    "        else:\n",
    "            letter_image_regions.append((x, y, w, h))\n",
    "\n",
    "    if len(letter_image_regions) != 4:\n",
    "        return None\n",
    "\n",
    "    letter_image_regions = sorted(letter_image_regions, key= lambda x: x[0])\n",
    "\n",
    "    return (img, letter_image_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab images to test against\n",
    "captcha_image_files = list(paths.list_images('generated_captcha_images'))\n",
    "captcha_image_files = np.random.choice(captcha_image_files, size=(10,), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "CAPTCHA text is BCUW\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "CAPTCHA text is 3DY6\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "CAPTCHA text is P547\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "CAPTCHA text is XZFJ\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "CAPTCHA text is SJFT\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "CAPTCHA text is VP4F\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "CAPTCHA text is UACZ\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "CAPTCHA text is QC7L\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "CAPTCHA text is TLDG\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "CAPTCHA text is 7K6H\n"
     ]
    }
   ],
   "source": [
    "for image_file in captcha_image_files:\n",
    "\n",
    "    preprocess_return = simple_captcha_test_preprocessing(image_file)\n",
    "\n",
    "    if preprocess_return == None:\n",
    "        continue\n",
    "    \n",
    "    (image, letter_image_regions) = preprocess_return\n",
    "\n",
    "    output = cv2.merge([image] * 3)\n",
    "    predictions = []\n",
    "\n",
    "    for letter_bounding_box in letter_image_regions:\n",
    "        (x, y, w, h) = letter_bounding_box\n",
    "\n",
    "        letter_image = image[y-2 : y + h + 2, x-2 : x+w+2]\n",
    "\n",
    "        letter_image = resize_to_fit(letter_image, 20, 20)\n",
    "\n",
    "        letter_image = np.expand_dims(letter_image, axis = 2)\n",
    "        letter_image = np.expand_dims(letter_image, axis = 0)\n",
    "        \n",
    "        prediction = new_model.predict(letter_image)\n",
    "\n",
    "        letter = lb.inverse_transform(prediction)[0]\n",
    "        predictions.append(letter)\n",
    "\n",
    "        cv2.rectangle(output, (x-2, y-2), (x+w+4, y+h+4), (0, 255, 0), 1)\n",
    "        cv2.putText(output, letter, (x - 5, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 255, 0), 2)\n",
    "\n",
    "    captcha_text = \"\".join(predictions)\n",
    "    print(\"CAPTCHA text is {}\".format(captcha_text))\n",
    "\n",
    "    cv2.imshow(\"Output\", output)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('functionalenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3e1b914758c0064768d5b7ded7793eb93afcfd32fe458b7f56104c45072a0c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
